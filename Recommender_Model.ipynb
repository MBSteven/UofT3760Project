{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edc768fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Below libraries are for feature representation using sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Below libraries are for similarity matrices using sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity  \n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7ee30f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/adon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/adon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/adon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c0bff65",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unexpected character found when decoding 'NaN'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m news_articles \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNews_Category_Dataset_v2.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlines\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/spacy/lib/python3.9/site-packages/pandas/util/_decorators.py:207\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/spacy/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/spacy/lib/python3.9/site-packages/pandas/io/json/_json.py:612\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m json_reader:\n\u001b[0;32m--> 612\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/spacy/lib/python3.9/site-packages/pandas/io/json/_json.py:744\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    742\u001b[0m         data \u001b[38;5;241m=\u001b[39m ensure_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m    743\u001b[0m         data_lines \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 744\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_combine_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_lines\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    746\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\n",
      "File \u001b[0;32m~/src/spacy/lib/python3.9/site-packages/pandas/io/json/_json.py:768\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    766\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 768\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    771\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[0;32m~/src/spacy/lib/python3.9/site-packages/pandas/io/json/_json.py:880\u001b[0m, in \u001b[0;36mParser.parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_numpy()\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 880\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_no_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    883\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/src/spacy/lib/python3.9/site-packages/pandas/io/json/_json.py:1133\u001b[0m, in \u001b[0;36mFrameParser._parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1129\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m DataFrame(\n\u001b[0;32m-> 1133\u001b[0m         \u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m     )\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1136\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1139\u001b[0m     }\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpected character found when decoding 'NaN'"
     ]
    }
   ],
   "source": [
    "news_articles = pd.read_json(\"News_Category_Dataset_v2.json\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caa664b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'news_articles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnews_articles\u001b[49m\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshort_description\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'news_articles' is not defined"
     ]
    }
   ],
   "source": [
    "news_articles.rename(columns={'short_description': 'description'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912c74e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_articles.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0027df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a297b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_articles = news_articles[news_articles['date'] >= pd.Timestamp(2018,1,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2e20ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_articles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2ea6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_articles = news_articles[news_articles['description'].apply(lambda x: len(x.split())>5)]\n",
    "print(\"Total number of articles after removal of description with short title:\", news_articles.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ba9872",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_articles.sort_values('description',inplace=True, ascending=False)\n",
    "duplicated_articles_series = news_articles.duplicated('description', keep = False)\n",
    "news_articles = news_articles[~duplicated_articles_series]\n",
    "print(\"Total number of articles after removing duplicates:\", news_articles.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff233f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_articles.index = range(news_articles.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a3a200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a new column containing both day of the week and month, it will be required later while recommending based on day of the week and month\n",
    "news_articles[\"day and month\"] = news_articles[\"date\"].dt.strftime(\"%a\") + \"_\" + news_articles[\"date\"].dt.strftime(\"%b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6e9bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_articles_temp = news_articles.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c02676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f60ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(news_articles_temp[\"description\"])):\n",
    "    string = \"\"\n",
    "    for word in news_articles_temp[\"description\"][i].split():\n",
    "        #print(word)\n",
    "        word = (\"\".join(e for e in word if e.isalnum()))\n",
    "        word = word.lower()\n",
    "        if not word in stop_words:\n",
    "          string += word + \" \"  \n",
    "    if(i%1000==0):\n",
    "      print(i)           # To track number of records processed\n",
    "    news_articles_temp.at[i,\"description\"] = string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dec62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77774342",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(news_articles_temp[\"description\"])):\n",
    "    string = \"\"\n",
    "    for w in word_tokenize(news_articles_temp[\"description\"][i]):\n",
    "        string += lemmatizer.lemmatize(w,pos = \"v\") + \" \"\n",
    "    news_articles_temp.at[i, \"description\"] = string.strip()\n",
    "    if(i%1000==0):\n",
    "        print(i)           # To track number of records processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034a8ba5",
   "metadata": {},
   "source": [
    "# TF-IDF method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee3a67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_description_vectorizer = TfidfVectorizer(min_df = 0)\n",
    "tfidf_description_features = tfidf_description_vectorizer.fit_transform(news_articles_temp['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06123dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_based_model(row_index, num_similar_items=6):\n",
    "    couple_dist = pairwise_distances(tfidf_description_features,tfidf_description_features[row_index])\n",
    "    indices = np.argsort(couple_dist.ravel())[0:num_similar_items]\n",
    "    df = pd.DataFrame({'publish_date': news_articles['date'][indices].values,\n",
    "               'description':news_articles['description'][indices].values,\n",
    "                'Euclidean similarity with the queried article': couple_dist[indices].ravel()})\n",
    "    #print(\"=\"*30,\"Queried article details\",\"=\"*30)\n",
    "    print('description : ',news_articles['description'][indices[0]])\n",
    "    #print(\"\\n\",\"=\"*25,\"Recommended articles : \",\"=\"*23)\n",
    "    \n",
    "    #return df.iloc[1:,1]\n",
    "    return df.iloc[1:,]\n",
    "tfidf_based_model(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48af1cd0",
   "metadata": {},
   "source": [
    "# Weighted similarity based on category, publish day and author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34f6746",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69ccc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_onehot_encoded = OneHotEncoder().fit_transform(np.array(news_articles_temp[\"category\"]).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd9eb68c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OneHotEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m publishingday_onehot_encoded \u001b[38;5;241m=\u001b[39m \u001b[43mOneHotEncoder\u001b[49m()\u001b[38;5;241m.\u001b[39mfit_transform(np\u001b[38;5;241m.\u001b[39marray(news_articles_temp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mday and month\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OneHotEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "publishingday_onehot_encoded = OneHotEncoder().fit_transform(np.array(news_articles_temp[\"day and month\"]).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c5cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_onehot_encoded = OneHotEncoder().fit_transform(np.array(news_articles_temp[\"authors\"]).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d9dcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_TFIDF_with_category_authors_and_publshing_day(row_index, num_similar_items, w1,w2,w3,w4): #headline_preference = True, category_preference = False):\n",
    "    w2v_dist  = pairwise_distances(tfidf_description_features,tfidf_description_features[row_index].reshape(1,-1))\n",
    "    category_dist = pairwise_distances(category_onehot_encoded, category_onehot_encoded[row_index]) + 1\n",
    "    authors_dist = pairwise_distances(authors_onehot_encoded, authors_onehot_encoded[row_index]) + 1\n",
    "    publishingday_dist = pairwise_distances(publishingday_onehot_encoded, publishingday_onehot_encoded[row_index]) + 1\n",
    "    weighted_couple_dist   = (w1 * w2v_dist +  w2 * category_dist + w3 * authors_dist + w4 * publishingday_dist)/float(w1 + w2 + w3 + w4)\n",
    "    indices = np.argsort(weighted_couple_dist.flatten())[0:num_similar_items].tolist()\n",
    "    df = pd.DataFrame({'publish_date': news_articles['date'][indices].values,\n",
    "                'headline_text':news_articles['headline'][indices].values,\n",
    "                'Weighted Euclidean similarity with the queried article': weighted_couple_dist[indices].ravel(),\n",
    "                'Word2Vec based Euclidean similarity': w2v_dist[indices].ravel(),\n",
    "                'Category based Euclidean similarity': category_dist[indices].ravel(),\n",
    "                'Authors based Euclidean similarity': authors_dist[indices].ravel(),   \n",
    "                'Publishing day based Euclidean similarity': publishingday_dist[indices].ravel(), \n",
    "                'Categoty': news_articles['category'][indices].values,\n",
    "                'Authors': news_articles['authors'][indices].values,\n",
    "                'Day and month': news_articles['day and month'][indices].values})\n",
    "    print(\"=\"*30,\"Queried article details\",\"=\"*30)\n",
    "    print('headline : ',news_articles['headline'][indices[0]])\n",
    "    print('Categoty : ', news_articles['category'][indices[0]])\n",
    "    print('Authors : ', news_articles['authors'][indices[0]])\n",
    "    print('Day and month : ', news_articles['day and month'][indices[0]])\n",
    "    print(\"\\n\",\"=\"*25,\"Recommended articles : \",\"=\"*23)\n",
    "    #return df.iloc[1:,[1,7,8,9]]\n",
    "    return df.iloc[1:, ]\n",
    "\n",
    "\n",
    "avg_TFIDF_with_category_authors_and_publshing_day(528,10,0.5,0.2,0.2,0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858215ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_articles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223b6e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b465a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
