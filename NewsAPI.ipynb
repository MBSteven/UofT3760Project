{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1609fbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "# Init\n",
    "newsapi = NewsApiClient(api_key='6bfed5d92d2047e496bd7fe940fe2336')\n",
    "\n",
    "#/v2/top-headlines\n",
    "# This API onli get the most recent hot articles titles etc.\n",
    "# top_headlines = newsapi.get_top_headlines(#q='TechCrunch')\n",
    "#                                          sources='google-news',\n",
    "#                                          #category='sports',\n",
    "#                                          #language='en',\n",
    "#                                          page_size=100)\n",
    "#                                          #country='us')\n",
    "\n",
    "# /v2/everything\n",
    "# Like line above described, this API fetch every article from the domain. \n",
    "all_articles = newsapi.get_everything(#q='market',\n",
    "                                      sources='bbc-news',\n",
    "                                      domains='http://www.bbc.co.uk/news',\n",
    "                                      from_param='2022-03-27',\n",
    "                                      to='2022-03-27',\n",
    "                                      language='en',\n",
    "                                      page_size=100)\n",
    "\n",
    "# /v2/top-headlines/sources\n",
    "# This API get a json file show all aviliable sources (source and domains)\n",
    "# You should run this API first to get an idea how which sites works for this API\n",
    "#sources = newsapi.get_sources()\n",
    "\n",
    "\n",
    "sources_data = top_headlines['articles']\n",
    "df = pd.DataFrame.from_dict(sources_data)\n",
    "\n",
    "#Save df to google bucket\n",
    "#I removed the GOOGLE_APPLICATION_CREDENTIALS so it will fail\n",
    "#Please feel free to update with your GCP credentials and buckets\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket('articlebuckets')\n",
    "    \n",
    "bucket.blob('upload_test/test.csv').upload_from_string(df.to_csv(), 'text/csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beac8e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_data = top_headlines['articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c82463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame.from_dict(sources_data,columns=['id','name','description','url','category','language','country'])\n",
    "df = pd.DataFrame.from_dict(sources_data)\n",
    "#df.to_csv('tech_bloomberg_2022-03-26_2022-03-26.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae5db7",
   "metadata": {},
   "source": [
    "## Few more thoughts how we should use this API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27bc117",
   "metadata": {},
   "source": [
    "    1. Limitation: 1000 calls every day. I think it would be enough if we use it wisely\n",
    "    2. I want to loop throught all english news websites every day 2-3 times to get all articles\n",
    "    3. Given that there is 1000 calls limit, we need decide which website and run frequency\n",
    "    4. Save all dfs to csv or parquet file to GCP. I think we should save all raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac8bcb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
